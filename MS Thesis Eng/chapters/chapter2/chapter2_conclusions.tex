\section*{Conclusions}
In this chapter, approximation problems based on linear regression were considered. For linear regression, basic methods for estimating coefficients were considered, and problems that may arise in the process of their estimation were also considered, such as insufficiently generalized ability due to lack of data or strong collinearity of data, leading to a high value of the condition number. Then, from the basic task of supervised learning, a transition was made to replacing the kernel in linear regression and possible replacements, such as the transformation of linear regression into the restoration of the image of the function in the Fourier and Chebyshev space, were considered. Specifically, these functions were considered in view of the fact that there are strong theorems guaranteeing convergence in pointwise and in the sense of the $L_2$ norm. Also, theorems on decreasing coefficients cannot be left unnoticed, since without loss of quality it is possible to limit these expansions in the future and not worry about subsequent effects. After the conclusions made that in the general case, with an arbitrary core, the linear regression model only builds an expansion in basic functions, it was suggested that a single-layer neural network is a similar expansion, with a pre-selected core - a sigmoid function. For this function, there is Tsybenkoâ€™s theorem guaranteeing the quality of approximation of arbitrary accuracy with an increase in the number of terms in the expansion. All these conclusions lead to the conclusion that to solve differential equations it remains only to correctly compose the optimization problem, then the solution will be guaranteed to be found, if it exists for the initial problem itself. Ideas and conclusions were borrowed from various works, and are indicated in the list of sources. However, it is worth noting that no work with a similar approach has been encountered before and this issue will be further worked out. In addition it is worth noting the second fact that the current chapter fully explains that to solve differential equations by the method using neural networks, it is enough to use single-layer networks, which significantly narrows the family of architectures for learning. An increase in depth is guaranteed to lead to an improvement in the solution with sufficient training time, however, effects appear associated with jumps in the objective function during training process due to the large number of local minima. For the tasks, we used the same optimization algorithm based on gradient descent and the influence of the algorithm parameters, as well as the choice of the algorithm itself on the quality of the solution and the rate of convergence, was not investigated, but most likely the results will differ and still this is important. At the end, solutions of typical problems for ODEs, system of ODE, and a one-dimensional partial differential equation were presented. Partial differential equations systems are presented in a next  chapter, since in addition to the introduced criteria for training a neural network, an important feature of the work is the solution of partial differential equations systems.
