\subsection{Another functions and expansion over them}
In case, when Generalized Fourier Series (GRS) is considered there are a lot of function can be used, for example\cite{abramowitz1965handbook}:
\begin{itemize}
	\item Gegenbauer polynomials
	\item Jacobi polynomials
	\item Romanovski polynomials
	\item Legendre polynomials
\end{itemize}

All have their own weight function and satisfy the GSR and potentially applicable for function approximation via the least-squares method and gradient-based methods with suitable penalty parameter (regularization term). In fact, during the approximation process (or supervised learning) includes fixation of the basis function, choice of the regularization. On the other hand, usage orthogonal functions for approximation or interpolation not restricted only GSR, actually, the linear regression model uses linearly-independent functions only.

\subsubsection{Function expansion over sigmoid function}
Let $\sigma = \dfrac{1}{1 + e^{-x}}$ and substitute to \eqref{eq:linear_expansion} instead of $\phi_i$ and apply an affine transformation to argument:
\begin{equation}
	\label{eq:sigmoidal_expansion}
	G(x) = \sum_{i = 1}^K \alpha_i \sigma(\beta_i x^j_i + \gamma_i)
\end{equation}
the expansion over the sigmoid functions is got. 
This expansion, in fact, one of the widely used in approximation process. First of all, there is a useful theorem that provides guarantees of the quality for approximation. 
\newpage
\begin{theorem}
	Let $\sigma = \dfrac{1}{1 + e^{-x}}$, then finite sums of the form:
	\begin{equation*}
		G(x) = \sum_{i = 1}^K \alpha_i \sigma(\beta_i x^j_i + \gamma_i)
	\end{equation*}
	are dense in $C(I_n\footnote{
	 Unit cube in $R^n$. The term unit cube or unit hypercube is also used for hypercubes, or "cubes" in $n$-dimensional spaces, for values of n other than 3 and edge length 1
	})$. In other words, given any $f \in C(I_n)\footnote{
		The space of continuous functions over $I_n$
	}, \epsilon > 0$, there is a sum, $G(x)$, of the above form, for which:
	 $\| G(x) - f(x) \| < \epsilon, \quad \forall x \in I_n$
\end{theorem}
In simple words, this theorem provides justification for the expansion of the function into the sigmoidal series, moreover, the quality of the approximation can be tremendously increased via increasing the terms in the series. By the way, the series \eqref{eq:sigmoidal_expansion} named neural network\footnote{Neural network, or Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with task-specific rules.} or perceptron\footnote{In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers or regressor.} with one hidden layer and sigmoidal activation function. 

Coefficients for the expansion can be found via the least-squares method or using the gradient-based methods which more suitable when talking about neural networks. The question of estimating them is a future section question, but there is a lot of special algorithms for it.